# -*- coding: utf-8 -*-
"""midecine reccomedation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pi6eIjXrpZqMJ9ksVpz6zmR2Ea2Hh5st
"""

import pandas as pd

data_for_fever = pd.read_csv("enhanced_fever_medicine_recommendation.csv")

x = data_for_fever.drop('Recommended_Medication',axis=1)
y = data_for_fever['Recommended_Medication']

print(y.unique())

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder

# label encoding for the target variable
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# identify and convert categorical data into numerical data
for col in x.select_dtypes(include=['object']):
    x[col] = label_encoder.fit_transform(x[col])


model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(x,y)

"""# as our data is small we will create more sample data and data augmentation

we are using **GaussianMixture** to generate more data and **smote** for balancing it
"""

from sklearn.mixture import GaussianMixture
import numpy as np
from sklearn.mixture import GaussianMixture
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE

# Fit GMM to existing data
gmm = GaussianMixture(n_components=3, random_state=42)
# n_components = no of unique targets
gmm.fit(x)

# Generate new synthetic samples
X_synthetic, _ = gmm.sample(50000)
y_synthetic = model.predict(X_synthetic)


x_comb = np.vstack([x, X_synthetic])
y_comb = np.hstack([y, y_synthetic])

df_comb = pd.DataFrame(x_comb, columns=x.columns)
df_comb['Recommended_Medication'] = y_comb

x = df_comb.drop('Recommended_Medication', axis=1)
y = df_comb['Recommended_Medication']

# using smote for data balancing
smote = SMOTE(random_state=42)
x,y = smote.fit_resample(x, y)

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model.fit(x_train,y_train)
y_pred = model.predict(x_test)
print(classification_report(y_test,y_pred))

"""# Using GridSearchCV to find best parameter for our model"""

from sklearn.model_selection import RandomizedSearchCV

parameters = {
    'n_estimators': [100, 200, 300],  # Number of trees in the forest
    'max_depth': [10, 20, 30, None],  # Maximum depth of each tree
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node
    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required in a leaf node
    'max_features': ['sqrt', 'log2', None]  # Number of features to consider at each split
}

grid_search = RandomizedSearchCV(estimator=model, param_distributions = parameters,
                               cv = 2, n_iter = 10, n_jobs=-1)

grid_search.fit(x_train, y_train)

# Get the best parameters and the corresponding score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

final_model = RandomForestClassifier(**best_params)
final_model.fit(x_train,y_train)
y_pred = final_model.predict(x_test)
print(classification_report(y_test,y_pred))

